#0202
gc()

#0206-COMMENT ALL SAVE(, WRITE.CSV(, REMOVE ALL LINES CONTAINING READ.XLS

#Set base working directory
baseWD<-"D:\\R_SCRIPTS\\0508"
classPath<-"D:\\Softwares\\RedshiftJDBC41-1.1.10.1010.jar"


#Set up Current time for database entry and log file names
currentTime<-format(Sys.time(), "%Y-%m-%d %H:%M:%S")
currentTime_fName<-gsub("-","_",currentTime)
currentTime_fName<-gsub(" ","_",currentTime_fName)
currentTime_fName<-gsub(":","_",currentTime_fName)

#Enable Logging
LogFile<-paste(baseWD,"\\Log\\",currentTime_fName,"_English_Combine_Text_Mining_Scoring_v16.0_Log.txt",sep="")
zz_Log <- file(LogFile, open="wt")
sink(zz_Log, type = "message")

ListFile<-paste(baseWD,"\\Log\\",currentTime_fName,"_English_Combine_Text_Mining_Scoring_v16.0_List.txt",sep="")
zz_List <- file(ListFile, open="wt")
sink(zz_List, type = "output")


#Set libraries
library("RSQLite")
library(sqldf)
library(plyr)
library(stringr)
library("RPostgreSQL")
library("RJDBC")
library(tm)   
library(SnowballC)
library(dplyr)
library(stringr)
library(stringdist)
library(zoo)
library(xlsx)
library(qdap)
library(parallel)
library(snow)
#library(Rmpi)
library(stringi)
library(foreach)
library(doSNOW)
library(bigmemory)
library(biganalytics)
library(Matrix)
library(koRpus)
library(hunspell)


#Set Memory Limit
memory.limit()
memory.limit(size=16000000000000)
memory.limit()

#Set up Database Connection
drv_custom1 <- JDBC(driverClass = "com.amazon.redshift.jdbc41.Driver", classPath=classPath)
 
con <- dbConnect(drv_custom1,
                                                url="jdbc:redshift://dapcnvyranalytics.czsqals6k9uo.us-east-1.redshift.amazonaws.com:5439/conveyoranalytics",
                                                dbname = "conveyoranalytics",
                                                schemaname="common_tables",
                                                user = "dapappadmin",
                                                port = 5439,
                                                host = "dapcnvyranalytics.czsqals6k9uo.us-east-1.redshift.amazonaws.com",
                                                password = "Adm1n@aw$")
 
 


#Garbage Collection
gc()

#######################################################################
####0206-Create Error Log Entry and read input data
#######################################################################

#Change the schema
#Change the schema
dbGetQuery(con, "show search_path;")
dbSendUpdate(con, "set search_path to common_tables,public;")
dbGetQuery(con, "show search_path;")


#Create default Error log row
code_name<-paste(baseWD,"\\Code\\English_Combine_Text_Mining_Scoring_v16.0.R",sep="")
code_description<-"English Combine Text"
run_start_datetime<-currentTime
run_end_datetime<-NA
run_status<-"RUNNING or FAILURE"
run_status_message<-paste("CHECK LOG : ",LogFile, "AND LIST : ",ListFile,SEP="")
errorLogEntry<-data.frame(cbind(code_name,code_description,run_start_datetime,run_status,run_status_message,run_end_datetime))

#Write default Error log row to DB
dbWriteTable(con, "t_error_log", 
             value = errorLogEntry, overwrite=FALSE,append = TRUE, row.names = FALSE)
dbCommit(con)


#######################################################################
####0206-Disconnect from database
#######################################################################

#Disconnect from database
dbDisconnect(con)

#Disable DB libraries because these interfere with normal SQLDF calls
detach(package:RPostgreSQL, unload = TRUE)
detach(package:RJDBC, unload = TRUE)


#######################################################################
#### THIS CODE COMBINES ALL THE 4 OUTPUTS
#######################################################################

#Read all 4 output files generated by previous codes
getwd()
setwd(paste(baseWD,"\\Interim",sep=""))

English_Short<-get(load("outputFinal7_English_Short.rda"))
head(English_Short)

English_Long<-get(load("outputFinal7_English_Long.rda"))
head(English_Long)


nrow(English_Short)
nrow(English_Long)


gc()
#######################################################################################
### ENGLISH
#######################################################################################

##Combine English
English_All<-rbind(English_Short,English_Long)
nrow(English_All)
nrow(English_All)==nrow(English_Short)+nrow(English_Long)

head(English_All)

##Sort
English_All2<-English_All[order(English_All$notification_number,English_All$N_Tokens,English_All$NLP_Cluster),]
head(English_All2,100)
names(English_All2)

####Pick only unique ones

#Create Vectors to compare
Noti_vector<-as.character(str_trim(English_All2$notification_number))
length(Noti_vector)==nrow(English_All2)

Cluster_vector<-toupper(as.character(str_trim(English_All2$NLP_Cluster)))
length(Cluster_vector)==nrow(English_All2)

Textlength_vector<-toupper(as.character(str_trim(English_All2$Textlength)))
length(Textlength_vector)==nrow(English_All2)

first<-numeric(nrow(English_All2))

first[1]=1
for(i in 2:nrow(English_All2)){
	if(paste(Noti_vector[i],Cluster_vector[i],sep="-")!=paste(Noti_vector[i-1],Cluster_vector[i-1],sep="-")){
		first[i]=1
	} else if (paste(Noti_vector[i],Cluster_vector[i],sep="-")==paste(Noti_vector[i-1],Cluster_vector[i-1],sep="-")){
		first[i]=0
		Textlength_vector[i]<-"BOTH"
		Textlength_vector[i-1]<-"BOTH"
	}
}

sum(first)
nrow(English_All2)

nrow(English_All2)-sum(first)
table(Textlength_vector)
sum(Textlength_vector=="BOTH")==2*(nrow(English_All2)-sum(first))

head(English_All2[(Textlength_vector=="BOTH"),"NLP_Cluster"],100)

#Bring in new text length column
English_All3<-cbind(English_All2,Textlength_vector)
head(English_All3,20)
English_All3<-English_All3[,!(names(English_All3)%in% "Textlength")]
names(English_All3)
colnames(English_All3)<-c(names(English_All3)[!(names(English_All3) %in% "Textlength_vector")],"Textlength")
names(English_All3)

table(English_All3$Textlength)
table(Textlength_vector)

#Keep only first record
English_All4<-English_All3[first==1,]
nrow(English_All4)
table(English_All4$Textlength)

nrow(English_All4)==sum(first)
sum(English_All4$Textlength=="BOTH")==nrow(English_All2)-sum(first)

##Find common Noti between long and short text
Short_Text_Noti_English_Vector<-as.character(English_All4[(English_All4$Textlength %in% c("SHORT","BOTH")),"notification_number"])
Long_Text_Noti_English_Vector<-as.character(English_All4[(English_All4$Textlength %in% c("LONG","BOTH")),"notification_number"])


Short_Text_Noti_English_Vector<-unique(Short_Text_Noti_English_Vector)
Long_Text_Noti_English_Vector<-unique(Long_Text_Noti_English_Vector)

length(Short_Text_Noti_English_Vector)
length(Long_Text_Noti_English_Vector)

Common_Text_Noti_English_Vector<-Short_Text_Noti_English_Vector[Short_Text_Noti_English_Vector %in% Long_Text_Noti_English_Vector]
length(Common_Text_Noti_English_Vector)

English_All4$isCommon_Noti_Short_Long<-ifelse(English_All4$notification_number %in% Common_Text_Noti_English_Vector,1,0)
table(English_All4$isCommon_Noti_Short_Long)
head(English_All4)
names(English_All4)
nrow(English_All4)

table(English_All4$Textlength)
table(English_All4$isCommon_Noti_Short_Long,English_All4$Textlength)

## Keep only tokens <=3
English_All5<-English_All4[as.numeric(as.character(English_All4$N_Tokens))<=3,]
nrow(English_All5)
table(English_All5$N_Tokens)

#0508- Bring Current timestamp 
head(English_All5)
English_All5$creation_date<-currentTime
head(English_All5)

#0508- Arrange columns
names(English_All5)
English_All5<-English_All5[,c("notification_number","notification_description","NLP_Cluster","N_Tokens","NLP_Name_Source","Textlength","Language","isCommon_Noti_Short_Long","notification_datetime","creation_date")]
head(English_All5)
setwd(paste(baseWD,"\\Interim",sep=""))
save(English_All5,file="English_All5.rda")


#######################################################################
####0206-Database Write
#######################################################################

#Set libraries
library("RPostgreSQL")
library("RJDBC")

#Set up Database Connection
drv_custom1 <- JDBC(driverClass = "com.amazon.redshift.jdbc41.Driver", classPath=classPath)
 
con <- dbConnect(drv_custom1,
                                                url="jdbc:redshift://dapcnvyranalytics.czsqals6k9uo.us-east-1.redshift.amazonaws.com:5439/conveyoranalytics",
                                                dbname = "conveyoranalytics",
                                                schemaname="common_tables",
                                                user = "dapappadmin",
                                                port = 5439,
                                                host = "dapcnvyranalytics.czsqals6k9uo.us-east-1.redshift.amazonaws.com",
                                                password = "Adm1n@aw$")
 
 


#################### UPDATE FINAL OUTPUT ######################################

#Change the schema
dbGetQuery(con, "show search_path;")
dbSendUpdate(con, "set search_path to naturallanguageprocessing,public;")
dbGetQuery(con, "show search_path;")

#English
setwd(paste(baseWD,"\\Interim",sep=""))
English_All5<-get(load("English_All5.rda"))


head(English_All5)
English_All5$notification_description<-NA
head(English_All5)

#0508- Arrange columns
English_All5<-English_All5[,c("notification_number","notification_description","NLP_Cluster","N_Tokens","NLP_Name_Source","Textlength","Language","isCommon_Noti_Short_Long","notification_datetime","creation_date")]

setwd(paste(baseWD,"\\Output",sep=""))
write.table(English_All5, "English_All5.txt",
            na = "",
            row.names = FALSE,
            col.names = TRUE,
            sep = "\t")


#################### UPDATE ERROR LOG ######################################

# Change run status to SUCCESS if writing of results and metadata is successful

#Change the schema
dbGetQuery(con, "show search_path;")
dbSendUpdate(con, "set search_path to common_tables,public;")
dbGetQuery(con, "show search_path;")

#Delete
returnDelete2<-dbSendUpdate(con,"delete from common_tables.eventsnamedparts_nlp where notification_number in (
            					select notification_number
						from common_tables.nlp_m_sap_notification_processed
						where  ((notification_datetime >= dateadd(day, -15,current_date) and notification_datetime <= current_date)
						or (notification_changed_date >= dateadd(day, -15,current_date) and notification_changed_date <= current_date))
 						and codingdescr='Event Report'
						and operation is not NULL and asset is not NULL and asset not in ('Pampa Norte','Escondida','')
						)")
returnCommit2<-dbCommit(con)

### WRITE FLAG FILE ###
FLAG_English_All5<-format(Sys.time(), "%Y-%m-%d %H:%M:%S")

setwd(paste(baseWD,"\\Output",sep=""))
write.table(FLAG_English_All5, "FLAG_English_All5.txt",
            na = "",
            row.names = FALSE,
            col.names = FALSE,
            sep = "\t")

dbSendUpdate(con,"update common_tables.t_error_log
			set
				run_status='SUCCESS',
				run_status_message='',
				run_end_datetime=(select SYSDATE)
			 where code_description='English Combine Text'
			 and run_start_datetime=(select max(run_start_datetime) from common_tables.t_error_log where code_description='English Combine Text')")
dbCommit(con)


#######################################################################
####0206-Remove existing metadata and last output file
#######################################################################
setwd(paste(baseWD,"\\Interim",sep=""))
list.files(getwd())
file.remove(list.files(getwd()))


